{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24479843-4bf0-40e7-9bb1-a2f6bb1bcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q biopython tqdm transformers tokenizers accelerate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c8c00-9eb9-47cb-a0cb-f1ff3c74e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties to evaluate\n",
    "traits = [\"charge_pH7\",\"gravy\",\"aromaticity\",\"instability_index\",\"mol_weight\",\"iso_point\"]\n",
    "\n",
    "# Traits to steer down\n",
    "steer_down_traits = {\"aromaticity\", \"instability_index\"}\n",
    "\n",
    "def is_steer_down(prop: str) -> bool:\n",
    "    return prop in steer_down_traits\n",
    "\n",
    "def get_score(seq: str, trait_name: str):\n",
    "    \"\"\"Compute Biopython-based property for a protein sequence.\"\"\"\n",
    "    try:\n",
    "        pa = ProteinAnalysis(seq)\n",
    "        fns = {\n",
    "            \"charge_pH7\":        lambda x: x.charge_at_pH(7.0),\n",
    "            \"gravy\":             lambda x: x.gravy(),\n",
    "            \"aromaticity\":       lambda x: x.aromaticity(),\n",
    "            \"instability_index\": lambda x: x.instability_index(),\n",
    "            \"mol_weight\":        lambda x: x.molecular_weight(),\n",
    "            \"iso_point\":         lambda x: x.isoelectric_point(),\n",
    "        }\n",
    "        return fns[trait_name](pa)\n",
    "    except Exception:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896e303-c6e6-4140-b3d2-61f330481e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from tokenizers import Tokenizer\n",
    "import torch, torch.nn.functional as F\n",
    "\n",
    "MODEL_NAME = \"hugohrban/progen2-large\"\n",
    "REV = \"main\"  # optionally pin to a commit hash for reproducibility\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    revision=REV,\n",
    ").eval().to(device)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(MODEL_NAME, revision=REV)\n",
    "tokenizer.no_padding()\n",
    "\n",
    "AA = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "print(f\"Loaded: {MODEL_NAME} on {device} (dtype={dtype})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2f7af-9967-46ea-bb5f-425a09b79a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def top_k_sample(logits: torch.Tensor, k: int = 0, temperature: float = 1.0) -> int:\n",
    "    \"\"\"Temperature + optional top-k sampling; returns sampled token id (int).\"\"\"\n",
    "    if temperature <= 0:\n",
    "        return torch.argmax(logits, dim=-1).item()\n",
    "    logits = logits / temperature\n",
    "    if k and k > 0:\n",
    "        topk = torch.topk(logits, k)\n",
    "        probs = F.softmax(topk.values, dim=-1)\n",
    "        idx_in_topk = torch.multinomial(probs, num_samples=1).item()\n",
    "        return topk.indices[idx_in_topk].item()\n",
    "    else:\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "def generate_sequence_progen2(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    length: int = 500,\n",
    "    temperature: float = 0.7,\n",
    "    top_k: int = 3,\n",
    "    max_steps: int = 5000,\n",
    ") -> str:\n",
    "    model_device = next(model.parameters()).device\n",
    "\n",
    "    prompt = \"1\"\n",
    "    ids = tokenizer.encode(prompt).ids\n",
    "    input_ids = torch.tensor(ids, dtype=torch.long, device=model_device)\n",
    "\n",
    "    seq_chars = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_steps):\n",
    "            # forward\n",
    "            logits = model(input_ids).logits[-1, :]  # last-step logits\n",
    "\n",
    "            # sample next token id\n",
    "            next_id = top_k_sample(logits, k=top_k, temperature=temperature)\n",
    "\n",
    "            # append and check stopping conditions\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([next_id], device=model_device)], dim=0)\n",
    "\n",
    "            tok = tokenizer.id_to_token(next_id)\n",
    "\n",
    "            # Stop if EOS-like '2'\n",
    "            if tok == \"2\":\n",
    "                break\n",
    "\n",
    "            if len(tok) == 1 and tok in AA:\n",
    "                seq_chars.append(tok)\n",
    "                if len(seq_chars) >= length:\n",
    "                    break\n",
    "\n",
    "    return \"\".join(seq_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed41d8-99e6-4f0d-8931-48ae54134ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "\n",
    "n_trials        = 1 \n",
    "n_samples       = 500\n",
    "sequence_length = 500\n",
    "temperature     = 0.7\n",
    "top_k           = 3\n",
    "seed            = 42\n",
    "\n",
    "# Loop over properties\n",
    "for property_label in tqdm(\n",
    "    traits, desc=\"Properties\", dynamic_ncols=True, mininterval=0.2, leave=True, file=sys.stdout\n",
    "):\n",
    "\n",
    "    trial_means = []\n",
    "    trial_stds  = []\n",
    "    all_scores  = []\n",
    "\n",
    "    # single trial — no inner chatter, only the per-property tqdm below\n",
    "    for _ in range(n_trials):\n",
    "        # seed per trial\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        \n",
    "        for _ in tqdm(\n",
    "            range(n_samples),\n",
    "            desc=f\"Generating ({property_label})\",\n",
    "            dynamic_ncols=True,\n",
    "            mininterval=0.1,\n",
    "            leave=False,\n",
    "            file=sys.stdout\n",
    "        ):\n",
    "            seq = generate_sequence_progen2(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                length=sequence_length,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "            )\n",
    "            try:\n",
    "                scores.append(get_score(seq, property_label))\n",
    "            except Exception:\n",
    "                scores.append(np.nan)\n",
    "\n",
    "        # per-trial stats\n",
    "        scores_arr = np.asarray(scores, dtype=float)\n",
    "        scores_arr = scores_arr[~np.isnan(scores_arr)]\n",
    "        trial_mean = float(np.mean(scores_arr)) if len(scores_arr) else float(\"nan\")\n",
    "        trial_std  = float(np.std(scores_arr, ddof=1)) if len(scores_arr) > 1 else 0.0\n",
    "\n",
    "        trial_means.append(trial_mean)\n",
    "        trial_stds.append(trial_std)\n",
    "        all_scores.append(scores_arr.tolist())\n",
    "\n",
    "    # CI\n",
    "    if n_trials == 1 and len(all_scores[0]) > 1:\n",
    "        per_sample = np.array(all_scores[0], dtype=float)\n",
    "        n   = len(per_sample)\n",
    "        mean_ps = float(np.mean(per_sample))\n",
    "        std_ps  = float(np.std(per_sample, ddof=1))\n",
    "        ci95    = 1.96 * (std_ps / np.sqrt(n))\n",
    "    else:\n",
    "        overall_mean = float(np.mean(trial_means)) if trial_means else float(\"nan\")\n",
    "        overall_std  = float(np.std(trial_means, ddof=1)) if len(trial_means) > 1 else 0.0\n",
    "        n = sum(len(x) for x in all_scores)\n",
    "        mean_ps, std_ps = overall_mean, overall_std\n",
    "        ci95 = 1.96 * (overall_std / np.sqrt(max(n_trials, 1)))\n",
    "\n",
    "    # outputting results\n",
    "    print(f\"[SUMMARY] {property_label}: mean={mean_ps:.6f} | std={std_ps:.6f} | 95% CI=±{ci95:.6f} | n={n}\", flush=True)\n",
    "\n",
    "    trial_stats = pd.DataFrame({\n",
    "        'trial': np.arange(1, n_trials+1),\n",
    "        'mean_score': trial_means,\n",
    "        'std_score': trial_stds\n",
    "    })\n",
    "    trial_stats.to_csv(f\"progen2_trial_stats_{property_label}.csv\", index=False)\n",
    "\n",
    "    # scoring\n",
    "    flat_scores = list(itertools.chain.from_iterable(all_scores))\n",
    "    pd.DataFrame({'score': flat_scores}).to_csv(f\"progen2_generated_scores_{property_label}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4209c9-864e-4106-91e0-5ee541150132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
