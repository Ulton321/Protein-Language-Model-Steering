{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5729316-6263-450f-8b0d-eded5d9f9418",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Vars\n",
    "TARGET_PROP = \"gravy\"      # charge_pH7, aromaticity, instability_index, gravy, iso_point\n",
    "N_SAMPLES   = 500          \n",
    "SEQ_LEN     = 500\n",
    "TOP_K       = 5\n",
    "TEMP        = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821011a-8277-4a7d-a454-ddf57b8d9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q biopython transformers tokenizers accelerate datasets tqdm\n",
    "\n",
    "import os, math, random, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float16  # runtime weights for base/negated\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Experiment params\n",
    "\n",
    "N_TRIALS   = 1\n",
    "N_SAMPLES  = 500\n",
    "SEQ_LEN    = 500\n",
    "TOP_K      = 5\n",
    "TEMP       = 1.0\n",
    "\n",
    "# Fine-tune params\n",
    "MAX_TOKENS   = 96     \n",
    "EPOCHS       = 1\n",
    "BSZ          = 1      \n",
    "GRAD_ACCUM   = 16    \n",
    "LR           = 2e-5\n",
    "WARMUP_STEPS = 50\n",
    "MAX_STEPS    = None\n",
    "\n",
    "SAVE_DIR = Path(\"/workspace/progen2_taskvec\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "AMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "print(f\"Device: {DEVICE}, dtype: {DTYPE}, samples/state: {N_SAMPLES}, length: {SEQ_LEN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b0b9c-5b8a-4cbb-9d17-a9e659737fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "MODEL_NAME = \"hugohrban/progen2-large\"\n",
    "\n",
    "# this is the HF \"tokenizers\" Tokenizer, as required by this model card\n",
    "tokenizer = Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.no_padding()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "base_model.eval()\n",
    "print(\"Loaded base ProGen2-large.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9df8d-12f3-47a4-86eb-b88acd740617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_property(seq: str, prop: str) -> float:\n",
    "    pa = ProteinAnalysis(seq)\n",
    "    if prop == \"mol_weight\":\n",
    "        return pa.molecular_weight()\n",
    "    if prop == \"charge_pH7\":\n",
    "        return pa.charge_at_pH(7.0)\n",
    "    if prop == \"aromaticity\":\n",
    "        return pa.aromaticity()\n",
    "    if prop == \"instability_index\":\n",
    "        return pa.instability_index()\n",
    "    if prop == \"gravy\":\n",
    "        return pa.gravy()\n",
    "    if prop == \"iso_point\":\n",
    "        return pa.isoelectric_point()\n",
    "    raise ValueError(f\"Unknown property: {prop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b3bf2-ccc4-4636-9509-bc0383925060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading dataset once...\")\n",
    "dataset = load_dataset(\"protolyze/plminterp\", split=\"train\")\n",
    "df_all = pd.DataFrame(dataset)\n",
    "assert \"Sequence\" in df_all.columns, \"Expected 'Sequence' column in protolyze/plminterp.\"\n",
    "\n",
    "def build_non_examples_for_property(df: pd.DataFrame, prop: str, pct: float = 0.20) -> pd.DataFrame:\n",
    "    # score entire df for the target property\n",
    "    scores = []\n",
    "    for s in tqdm(df[\"Sequence\"], desc=f\"Scoring ({prop})\"):\n",
    "        try:\n",
    "            scores.append(score_property(s, prop))\n",
    "        except Exception:\n",
    "            scores.append(np.nan)\n",
    "    scored = df.copy()\n",
    "    scored[\"score\"] = scores\n",
    "    scored = scored.dropna(subset=[\"score\"]).reset_index(drop=True)\n",
    "\n",
    "    # partition\n",
    "    if prop in [\"aromaticity\", \"instability_index\"]:\n",
    "        # steer downward = take top scoring as non-examples\n",
    "        scored = scored.sort_values(\"score\", ascending=False)\n",
    "    else:\n",
    "        # steer upward = take bottom scoring as non-examples\n",
    "        scored = scored.sort_values(\"score\", ascending=True)\n",
    "\n",
    "    n = int(pct * len(scored))\n",
    "    non_examples = scored.head(max(1, n)).reset_index(drop=True)\n",
    "    return non_examples[[\"Sequence\", \"score\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b9a01-8fd6-44e9-a8e5-efc714e300ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProGenCausalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, tokenizer, max_tokens=256):\n",
    "        self.sequences = sequences\n",
    "        self.tok = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = str(self.sequences[idx])\n",
    "        ids = self.tok.encode(seq).ids\n",
    "        ids = ids[: self.max_tokens]\n",
    "        attn = [1] * len(ids)\n",
    "\n",
    "        pad_len = self.max_tokens - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids  = ids  + [0] * pad_len\n",
    "            attn = attn + [0] * pad_len\n",
    "\n",
    "        input_ids      = torch.tensor(ids,  dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attn, dtype=torch.long)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100  # ignore pads\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "def _unfreeze_last_n_blocks(model, n_last: int = 4):\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # find blocks list\n",
    "    blocks = None\n",
    "    containers = []\n",
    "    for attr in [\"transformer\", \"model\", \"gpt_neox\", \"backbone\"]:\n",
    "        if hasattr(model, attr):\n",
    "            containers.append(getattr(model, attr))\n",
    "    for c in containers:\n",
    "        for name in [\"h\", \"layers\", \"blocks\"]:\n",
    "            if hasattr(c, name):\n",
    "                blocks = getattr(c, name)\n",
    "                break\n",
    "        if blocks is not None:\n",
    "            break\n",
    "\n",
    "    # unfreeze last n blocks\n",
    "    if blocks is not None:\n",
    "        for b in (list(blocks)[-n_last:] if hasattr(blocks, \"__iter__\") else []):\n",
    "            for p in b.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    if hasattr(model, \"lm_head\"):\n",
    "        for p in model.lm_head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "def finetune_on_non_examples(base_model, sequences, epochs=1, max_tokens=96, lr=2e-5, bsz=1, grad_accum=16,\n",
    "                             warmup_steps=50, max_steps=None, save_path=None, n_last_trainable_blocks: int = 4):\n",
    "\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    import bitsandbytes as bnb\n",
    "\n",
    "    # Load training model in bf16 (A40 supports bfloat16 well)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    if getattr(model.config, \"use_cache\", None) is not None:\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    # freeze all but last few blocks\n",
    "    _unfreeze_last_n_blocks(model, n_last=n_last_trainable_blocks)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ds = ProGenCausalDataset(sequences, tokenizer, max_tokens=max_tokens)\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        ds, batch_size=bsz, shuffle=True, drop_last=True, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # 8-bit Adam\n",
    "    opt = bnb.optim.Adam8bit(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    total_steps = max_steps or (epochs * math.ceil(len(ds) / (bsz * grad_accum)))\n",
    "    def lr_lambda(step):\n",
    "        if warmup_steps and step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        if warmup_steps and step >= warmup_steps:\n",
    "            return max(0.0, float(total_steps - step) / float(max(1, total_steps - warmup_steps)))\n",
    "        return max(0.0, float(total_steps - step) / float(max(1, total_steps)))\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "\n",
    "    # amp in bf16 path \n",
    "    step = 0\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dl, desc=f\"Finetuning (epoch {epoch+1}/{epochs})\", leave=False)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(model.device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.bfloat16):\n",
    "                out = model(**batch)\n",
    "                loss = out.loss / grad_accum\n",
    "\n",
    "            loss.backward()\n",
    "            if (i + 1) % grad_accum == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    [p for p in model.parameters() if p.requires_grad], 1.0\n",
    "                )\n",
    "                opt.step(); opt.zero_grad(set_to_none=True); sched.step(); step += 1\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{out.loss.item():.4f}\", \"step\": step})\n",
    "            if max_steps and step >= max_steps:\n",
    "                break\n",
    "        if max_steps and step >= max_steps:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    if save_path:\n",
    "        save_path = Path(save_path); save_path.mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(save_path)\n",
    "        print(f\"Saved finetuned model to: {save_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba9405-2732-467c-9463-6ecd3551eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_negated_model(base_model, finetuned_model):\n",
    "    with torch.no_grad():\n",
    "        base_sd = {k: v.to(\"cpu\") for k, v in base_model.state_dict().items()}\n",
    "        ft_sd   = {k: v.to(\"cpu\") for k, v in finetuned_model.state_dict().items()}\n",
    "        neg_sd  = {}\n",
    "        for k, bv in base_sd.items():\n",
    "            fv = ft_sd.get(k, None)\n",
    "            if fv is not None and fv.shape == bv.shape:\n",
    "                delta = fv - bv\n",
    "                neg_sd[k] = (bv - delta).to(DTYPE)\n",
    "            else:\n",
    "                neg_sd[k] = bv.to(DTYPE)\n",
    "\n",
    "    neg_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    missing, unexpected = neg_model.load_state_dict(neg_sd, strict=False)\n",
    "    neg_model.eval()\n",
    "    print(\"Negated task-vector model built.\",\n",
    "          f\"\\n missing keys: {len(missing)} | unexpected keys: {len(unexpected)}\")\n",
    "    return neg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609a6e5-2d70-40db-9723-79aee82e3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build amino-acid token id list once\n",
    "AA_IDs = []\n",
    "for aa in AMINO_ACIDS:\n",
    "    tid = tokenizer.token_to_id(aa)\n",
    "    if tid is not None:\n",
    "        AA_IDs.append(tid)\n",
    "AA_IDs = torch.tensor(AA_IDs, device=base_model.device if hasattr(base_model, 'device') else 0)\n",
    "\n",
    "def generate_protein_sequence(model, length: int = 100, temperature: float = 1.0, top_k: int = 5, seed_prompt: str = \"M\") -> str:\n",
    "    ids = tokenizer.encode(seed_prompt).ids\n",
    "    x = torch.tensor(ids, device=model.device, dtype=torch.long).unsqueeze(0) \n",
    "    with torch.no_grad():\n",
    "        steps = max(0, length - len(seed_prompt))\n",
    "        for _ in range(steps):\n",
    "            logits = model(x).logits[:, -1, :]  \n",
    "            aa_logits = logits.index_select(dim=-1, index=AA_IDs) \n",
    "            aa_logits = aa_logits / max(1e-6, temperature)\n",
    "\n",
    "            k = min(top_k, aa_logits.shape[-1])\n",
    "            topk_vals, topk_idx = torch.topk(aa_logits, k=k, dim=-1)\n",
    "            probs = F.softmax(topk_vals, dim=-1)\n",
    "            sample_local = torch.multinomial(probs, num_samples=1)\n",
    "            next_token_global = AA_IDs[topk_idx.gather(1, sample_local)].view(1,1)\n",
    "            x = torch.cat([x, next_token_global], dim=1)\n",
    "\n",
    "    tokens = [tokenizer.id_to_token(t) for t in x[0].tolist()]\n",
    "    seq_only = \"\".join([t for t in tokens if len(t) == 1 and t in AMINO_ACIDS])\n",
    "    if len(seq_only) < length: seq_only = seq_only + (\"A\" * (length - len(seq_only)))\n",
    "    elif len(seq_only) > length: seq_only = seq_only[:length]\n",
    "    return seq_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1582352-c1f5-45ab-b932-978c7d2ac9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    TARGET_PROP \n",
    "except NameError:\n",
    "    TARGET_PROP = os.environ.get(\"PROP\", \"\").strip()\n",
    "    if not TARGET_PROP:\n",
    "        TARGET_PROP = \"gravy\"  # safe default for ad-hoc runs\n",
    "\n",
    "assert TARGET_PROP in {\"charge_pH7\",\"aromaticity\",\"instability_index\",\"gravy\",\"iso_point\"}, \\\n",
    "       f\"TARGET_PROP='{TARGET_PROP}' must be one of the 5 (excluding 'mol_weight').\"\n",
    "\n",
    "PROPERTIES = [TARGET_PROP] \n",
    "print(f\"[CONFIG] Running single property: {TARGET_PROP}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9d78e-2d44-451c-86b6-652c9770b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT -> generate -> score TARGET_PROP only\n",
    "prop = PROPERTIES[0]\n",
    "print(f\"\\n=== PROPERTY (task-vector target): {prop} ===\")\n",
    "\n",
    "# Non-examples for this prop\n",
    "non_examples = build_non_examples_for_property(df_all, prop, pct=0.20)\n",
    "non_examples.to_csv(SAVE_DIR / f\"non_examples_{prop}.csv\", index=False)\n",
    "train_seqs = non_examples[\"Sequence\"].tolist()\n",
    "print(f\"Non-examples for {prop}: {len(train_seqs)}\")\n",
    "\n",
    "# free the GPU before FT, move base to CPU\n",
    "base_model.to(\"cpu\"); torch.cuda.empty_cache()\n",
    "\n",
    "# Fine-tune\n",
    "ft_path = SAVE_DIR / f\"ft_{prop}\"\n",
    "ft_model = finetune_on_non_examples(\n",
    "    base_model=None,        \n",
    "    sequences=train_seqs,\n",
    "    epochs=EPOCHS, max_tokens=MAX_TOKENS,\n",
    "    lr=LR, bsz=BSZ, grad_accum=GRAD_ACCUM,\n",
    "    warmup_steps=WARMUP_STEPS, max_steps=MAX_STEPS,\n",
    "    save_path=ft_path\n",
    ")\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, trust_remote_code=True, torch_dtype=DTYPE, device_map=\"auto\"\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "# Build negated model (TA)\n",
    "neg_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, trust_remote_code=True, torch_dtype=DTYPE, device_map={\"\": \"cpu\"}\n",
    ")\n",
    "# compute negated state dict\n",
    "with torch.no_grad():\n",
    "    base_sd = {k: v.to(\"cpu\") for k, v in base_model.state_dict().items()}\n",
    "    ft_sd   = {k: v.to(\"cpu\") for k, v in ft_model.state_dict().items()}\n",
    "    neg_sd  = {}\n",
    "    for k, bv in base_sd.items():\n",
    "        fv = ft_sd.get(k, None)\n",
    "        if fv is not None and fv.shape == bv.shape:\n",
    "            delta = fv - bv\n",
    "            neg_sd[k] = (bv - delta).to(DTYPE)\n",
    "        else:\n",
    "            neg_sd[k] = bv.to(DTYPE)\n",
    "missing, unexpected = neg_model.load_state_dict(neg_sd, strict=False)\n",
    "neg_model.to(\"cuda\"); neg_model.eval()\n",
    "print(\"Negated task-vector model built.\",\n",
    "      f\"\\n missing keys: {len(missing)} | unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "def gen_and_score_single_property(model, state_tag, prop, n_samples=N_SAMPLES, seq_len=SEQ_LEN, temp=TEMP, top_k=TOP_K):\n",
    "    seqs = []\n",
    "    for _ in tqdm(range(n_samples), desc=f\"Generating ({prop}, state={state_tag})\"):\n",
    "        seqs.append(generate_protein_sequence(model, length=seq_len, temperature=temp, top_k=top_k, seed_prompt=\"M\"))\n",
    "\n",
    "    scores = []\n",
    "    for s in seqs:\n",
    "        try: scores.append(score_property(s, prop))\n",
    "        except Exception: scores.append(np.nan)\n",
    "\n",
    "    df = pd.DataFrame({\"sequence\": seqs, prop: scores})\n",
    "    vals = df[prop].dropna().to_numpy(dtype=np.float64)\n",
    "    n = vals.size\n",
    "    if n > 1:\n",
    "        mean = float(vals.mean()); std = float(vals.std(ddof=1)); ci = 1.96 * std / math.sqrt(n)\n",
    "    elif n == 1:\n",
    "        mean, std, ci = float(vals[0]), 0.0, 0.0\n",
    "    else:\n",
    "        mean, std, ci = 0.0, 0.0, 0.0\n",
    "\n",
    "    print(f\"[SUMMARY] state={state_tag} | {prop}: mean={mean:.6f} | std={std:.6f} | 95% CI=Â±{ci:.6f} | n={n}\")\n",
    "    return df, mean, std, ci, n\n",
    "\n",
    "# Scoring/evaluation\n",
    "summaries = []\n",
    "for state_tag, mdl in [(\"base\", base_model), (\"finetuned\", ft_model), (\"negated\", neg_model)]:\n",
    "    df_metrics, mean, std, ci, n = gen_and_score_single_property(mdl, state_tag, prop)\n",
    "    out_prefix = SAVE_DIR / f\"{prop}__{state_tag}\"\n",
    "    pd.DataFrame({\"sequence\": df_metrics[\"sequence\"]}).to_csv(f\"{out_prefix}__sequences.csv\", index=False)\n",
    "    df_metrics.to_csv(f\"{out_prefix}__metrics.csv\", index=False)\n",
    "    summaries.append({\"taskvec_target\": prop, \"state\": state_tag, \"mean\": mean, \"std\": std, \"ci95\": ci, \"n\": n})\n",
    "\n",
    "pd.DataFrame(summaries).to_csv(SAVE_DIR / f\"summary_{prop}_all_states.csv\", index=False)\n",
    "print(f\"\\nSaved summary: {SAVE_DIR / f'summary_{prop}_all_states.csv'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
