{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2194b7-9c1a-4b60-9fc6-1547514284bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy biopython tqdm transformers accelerate sentencepiece peft\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15febe03-e4b4-44f4-939c-93f025863362",
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = [\"charge_pH7\",\"gravy\",\"aromaticity\",\"instability_index\",\"mol_weight\",\"iso_point\"]\n",
    "\n",
    "steer_down_traits = {\"aromaticity\", \"instability_index\"}\n",
    "\n",
    "def is_steer_down(prop: str) -> bool:\n",
    "    return prop in steer_down_traits\n",
    "\n",
    "def get_score(seq: str, trait_name: str):\n",
    "    \"\"\"Compute Biopython-based property for a protein sequence.\"\"\"\n",
    "    try:\n",
    "        pa = ProteinAnalysis(seq)\n",
    "        fns = {\n",
    "            \"charge_pH7\":        lambda x: x.charge_at_pH(7.0),\n",
    "            \"gravy\":             lambda x: x.gravy(),\n",
    "            \"aromaticity\":       lambda x: x.aromaticity(),\n",
    "            \"instability_index\": lambda x: x.instability_index(),\n",
    "            \"mol_weight\":        lambda x: x.molecular_weight(),\n",
    "            \"iso_point\":         lambda x: x.isoelectric_point(),\n",
    "        }\n",
    "        return fns[trait_name](pa)\n",
    "    except Exception:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bacb18-5a37-41a5-a556-15698c3f3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"GreatCaptainNemo/ProLLaMA\"\n",
    "REV = \"main\"  \n",
    "ADAPTER_PATH = None \n",
    "\n",
    "dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load base model\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    revision=REV,\n",
    ")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, revision=REV)\n",
    "# Ensure PAD exists\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Optionally load a LoRA adapter\n",
    "if ADAPTER_PATH:\n",
    "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "    model.to(device).eval()\n",
    "    print(f\"Loaded LoRA adapter from: {ADAPTER_PATH}\")\n",
    "\n",
    "AA = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "print(f\"Loaded ProLLaMA on {device} (dtype={dtype})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16972c23-7c48-42e8-bb05-a2c48fee28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "SUPERFAMILY_PROMPT = \"Ankyrin repeat-containing domain superfamily\"\n",
    "\n",
    "def extract_aa(text: str, max_len: int) -> str:\n",
    "    filtered = \"\".join(ch for ch in text if ch in AA)\n",
    "    return filtered[:max_len]\n",
    "\n",
    "def generate_sequence_prollama(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    length: int = 500,\n",
    "    temperature: float = 0.7,\n",
    "    top_k: int = 40,\n",
    "    top_p: float = 0.9,\n",
    "    max_new_tokens: int = 512,\n",
    "    repetition_penalty: float = 1.2,\n",
    "    superfamily: str = SUPERFAMILY_PROMPT,\n",
    ") -> str:\n",
    "\n",
    "    model_device = next(model.parameters()).device\n",
    "    prompt = f\"[Generate by superfamily] Superfamily=<{superfamily}>\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(model_device)\n",
    "    attn_mask = inputs[\"attention_mask\"].to(model_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract AA-only and cap to `length`\n",
    "    return extract_aa(decoded, max_len=length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d51756-ad48-4756-8f90-8ab4ff4a510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "\n",
    "n_trials        = 1\n",
    "n_samples       = 500\n",
    "sequence_length = 500\n",
    "temperature     = 0.7\n",
    "top_k           = 40     \n",
    "top_p           = 0.9\n",
    "seed            = 42\n",
    "\n",
    "for property_label in tqdm(\n",
    "    traits, desc=\"Properties\", dynamic_ncols=True, mininterval=0.2, leave=True, file=sys.stdout\n",
    "):\n",
    "    trial_means, trial_stds, all_scores = [], [], []\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        scores = []\n",
    "        for _ in tqdm(\n",
    "            range(n_samples),\n",
    "            desc=f\"Generating ({property_label})\",\n",
    "            dynamic_ncols=True,\n",
    "            mininterval=0.1,\n",
    "            leave=False,\n",
    "            file=sys.stdout\n",
    "        ):\n",
    "            seq = generate_sequence_prollama(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                length=sequence_length,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "            try:\n",
    "                scores.append(get_score(seq, property_label))\n",
    "            except Exception:\n",
    "                scores.append(np.nan)\n",
    "\n",
    "        arr = np.asarray(scores, dtype=float)\n",
    "        arr = arr[~np.isnan(arr)]\n",
    "        trial_mean = float(np.mean(arr)) if len(arr) else float(\"nan\")\n",
    "        trial_std  = float(np.std(arr, ddof=1)) if len(arr) > 1 else 0.0\n",
    "\n",
    "        trial_means.append(trial_mean)\n",
    "        trial_stds.append(trial_std)\n",
    "        all_scores.append(arr.tolist())\n",
    "\n",
    "    # CI (n_trials=1 => from per-sample std)\n",
    "    if n_trials == 1 and all_scores and len(all_scores[0]) > 1:\n",
    "        per_sample = np.array(all_scores[0], dtype=float)\n",
    "        n    = len(per_sample)\n",
    "        mean_ps = float(np.mean(per_sample))\n",
    "        std_ps  = float(np.std(per_sample, ddof=1))\n",
    "        ci95    = 1.96 * (std_ps / np.sqrt(n))\n",
    "    else:\n",
    "        overall_mean = float(np.mean(trial_means)) if trial_means else float(\"nan\")\n",
    "        overall_std  = float(np.std(trial_means, ddof=1)) if len(trial_means) > 1 else 0.0\n",
    "        n = sum(len(x) for x in all_scores) if all_scores else 0\n",
    "        mean_ps, std_ps = overall_mean, overall_std\n",
    "        ci95 = 1.96 * (overall_std / np.sqrt(max(n_trials, 1))) if n_trials > 0 else 0.0\n",
    "\n",
    "    print(f\"[SUMMARY] {property_label}: mean={mean_ps:.6f} | std={std_ps:.6f} | 95% CI=Â±{ci95:.6f} | n={n}\", flush=True)\n",
    "\n",
    "    # Save\n",
    "    pd.DataFrame({'trial': np.arange(1, n_trials+1), 'mean_score': trial_means, 'std_score': trial_stds}) \\\n",
    "      .to_csv(f\"prollama_trial_stats_{property_label}.csv\", index=False)\n",
    "\n",
    "    flat_scores = list(itertools.chain.from_iterable(all_scores))\n",
    "    pd.DataFrame({'score': flat_scores}).to_csv(f\"prollama_generated_scores_{property_label}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
